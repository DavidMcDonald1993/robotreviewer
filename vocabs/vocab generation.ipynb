{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of updated vocabulary files for minimap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import csv\n",
    "import spacy\n",
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
    "from spacy.tokens import Doc\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expressions and text processing functions\n",
    "\n",
    "import re\n",
    "\n",
    "with open('../robotreviewer/data/minimap/prepositions_conjunctions.txt', 'r') as f:\n",
    "    prep_conj = [l.strip() for l in f]\n",
    "\n",
    "prep_conj_re = re.compile(r'\\b({})\\b'.format('|'.join(prep_conj)))\n",
    "nos_ignore = re.compile(r'\\bNOS\\b') # note do after lowercase\n",
    "pos_ignore = re.compile(r\"(?<=\\w)(\\'s?)\\b\")\n",
    "left_paren = re.compile(r\"^\\[(X|V|D|M|EDTA|SO|Q)\\]\")\n",
    "paren = re.compile(r\"[\\(\\[]\\w+[\\)\\]]\")\n",
    "strip_space = re.compile(r\"\\s+\")\n",
    "\n",
    "def remove_nos(text):\n",
    "    return nos_ignore.sub(' ', text)\n",
    "\n",
    "def remove_pos(text):\n",
    "    return pos_ignore.sub('', text)\n",
    "\n",
    "def syn_uninv(text):\n",
    "    try:\n",
    "        inversion_point = text.index(', ')\n",
    "    except ValueError:\n",
    "        # not found\n",
    "        return text\n",
    "    \n",
    "    if inversion_point+2 == len(text):\n",
    "        # i.e. if the ', ' is at the end of the string\n",
    "        return text\n",
    "    \n",
    "    if prep_conj_re.search(text[inversion_point+2:]):\n",
    "        return text\n",
    "    else:\n",
    "        return text[inversion_point+2:] + \" \" + text[:inversion_point]\n",
    "    \n",
    "def ne_parentheticals(text_str):\n",
    "    text_str = left_paren.sub('', text_str)\n",
    "    text_str = paren.sub('', text_str)\n",
    "    return text_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines\n",
    "\n",
    "def minimap(text_str, chunks=False):\n",
    "    return matcher(pipeline(text_str, umls_mode=False), chunks=chunks)\n",
    "\n",
    "\n",
    "def pipeline(text_str, umls_mode=True):\n",
    "        \n",
    "    # 1. removal of parentheticals\n",
    "    if umls_mode:\n",
    "        text_str = ne_parentheticals(text_str)\n",
    "    \n",
    "    # hyphens to spaces\n",
    "    text_str = text_str.replace('-', ' ')\n",
    "    # 3. conversion to lowercase\n",
    "    # text_str = text_str.lower()\n",
    "    # 2. syntactic uninverstion\n",
    "    if umls_mode:\n",
    "        text_str = syn_uninv(text_str)\n",
    "    # 4. stripping of possessives\n",
    "    text_str = remove_pos(text_str)\n",
    "    # strip NOS's\n",
    "    if umls_mode:\n",
    "        text_str = remove_nos(text_str)\n",
    "    # last... remove any multiple spaces, or starting/ending with space\n",
    "    text_str = strip_space.sub(' ', text_str)    \n",
    "    text_str = text_str.strip()\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first generate str to CUI map\n",
    "str_to_cui_full = defaultdict(list)\n",
    "\n",
    "with open('umls_full_index.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in tqdm.tqdm(reader):\n",
    "        if row['sab'] in ['MSH', 'SNOMEDCT_US', 'MDR', \"ATC\", \"RXNORM\", \"ICD10\"]:\n",
    "            # just keep those which are in the Cochrane vocabs\n",
    "            doc = nlp(pipeline(row['str'], umls_mode=True).lower())\n",
    "            str_to_cui_full[' '.join(t.lemma_ for t in doc)].append(row['cui'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_cui = {}\n",
    "for k, v in str_to_cui_full.items():\n",
    "    str_to_cui[k] = list(set(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('str_to_cui.pck', 'wb') as f:\n",
    "    pickle.dump(str_to_cui_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now CUI to preferred term map\n",
    "\n",
    "df = pd.read_csv('cui_str.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cui_to_pstr = defaultdict(dict)\n",
    "for i, r in tqdm.tqdm(df.iterrows()):\n",
    "    cui_to_pstr[r['cui']][r['sab']] = r['str']\n",
    "order = [\"RXNORM\", \"MSH\", \"SNOMEDCT_US\", \"ICD10\", \"MDR\", \"ATC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_to_str = {}\n",
    "\n",
    "for k, v in cui_to_pstr.items():\n",
    "    for p in order:\n",
    "        if p in v:\n",
    "            cui_to_str[k] = v[p]\n",
    "            break\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cui_to_str.pck', 'wb') as f:\n",
    "    pickle.dump(cui_to_str, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(graph_data.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_data = pd.read_csv('cui_graph.csv', sep='\\t')\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(((r['cui2'], r['cui1']) for i, r in tqdm.tqdm(graph_data.iterrows())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cui_subtrees.pck', 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
